# K-Docs - Architecture IA avec Fallback

## Vue d'ensemble

K-Docs utilise une strat√©gie de **fallback intelligent** pour les services IA :

```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ                    K-DOCS AI STRATEGY                           ‚îÇ
‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§
‚îÇ                                                                  ‚îÇ
‚îÇ   Priority 1: CLAUDE API                                         ‚îÇ
‚îÇ   ‚îú‚îÄ Configured? ‚îÄ‚îÄ‚ñ∫ YES ‚îÄ‚îÄ‚ñ∫ Use Claude (best quality)          ‚îÇ
‚îÇ   ‚îî‚îÄ NO ‚Üì                                                        ‚îÇ
‚îÇ                                                                  ‚îÇ
‚îÇ   Priority 2: OLLAMA (Local)                                     ‚îÇ
‚îÇ   ‚îú‚îÄ Running? ‚îÄ‚îÄ‚ñ∫ YES ‚îÄ‚îÄ‚ñ∫ Use Ollama (good quality, free)       ‚îÇ
‚îÇ   ‚îî‚îÄ NO ‚Üì                                                        ‚îÇ
‚îÇ                                                                  ‚îÇ
‚îÇ   Priority 3: Rules-only mode                                    ‚îÇ
‚îÇ   ‚îî‚îÄ Pattern matching only (basic classification)                ‚îÇ
‚îÇ                                                                  ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

## Composants

### 1. Claude API (Priorit√© 1)
- **Qualit√©** : Excellente
- **Co√ªt** : Payant (~$3/million tokens)
- **Fonctionnalit√©s** : Classification, extraction, r√©sum√©, chat
- **Configuration** : `claude_api_key.txt` ou `config.php`

### 2. Ollama (Priorit√© 2 - Fallback)
- **Qualit√©** : Bonne √† acceptable
- **Co√ªt** : Gratuit (local)
- **Fonctionnalit√©s** : Classification, extraction, r√©sum√©, chat, embeddings
- **Mod√®les recommand√©s** :
  - LLM : `llama3.2` (classification, chat)
  - Embeddings : `nomic-embed-text` (recherche s√©mantique)

### 3. Rules-only (Priorit√© 3)
- **Qualit√©** : Basique
- **Fonctionnalit√©s** : Matching patterns uniquement
- **Usage** : Quand aucune IA n'est disponible

## Installation Ollama (fallback)

```bash
# 1. Installer Ollama
# Windows: https://ollama.ai/download
# Linux: curl -fsSL https://ollama.ai/install.sh | sh

# 2. T√©l√©charger les mod√®les
ollama pull llama3.2          # ~2GB - LLM pour classification
ollama pull nomic-embed-text  # ~275MB - Embeddings pour recherche

# 3. V√©rifier
ollama list
curl http://localhost:11434/api/tags
```

## Configuration

### config.php
```php
return [
    // Claude (prioritaire si configur√©)
    'claude' => [
        'api_key' => '', // Laisser vide pour utiliser Ollama
        'model' => 'claude-sonnet-4-20250514',
    ],
    
    // Ollama (fallback automatique)
    'api' => [
        'ollama_url' => 'http://localhost:11434',
    ],
    'ollama' => [
        'model' => 'llama3.2', // Mod√®le LLM par d√©faut
    ],
    
    // Embeddings (recherche s√©mantique)
    'embeddings' => [
        'enabled' => true,
        'provider' => 'ollama', // ou 'openai'
        'ollama_model' => 'nomic-embed-text',
    ],
];
```

## API Endpoints

### GET /api/ai/status
Retourne le statut des providers :
```json
{
  "active_provider": "ollama",
  "ai_available": true,
  "claude": {
    "available": false,
    "configured": false
  },
  "ollama": {
    "available": true,
    "url": "http://localhost:11434",
    "model": "llama3.2",
    "models": ["llama3.2:latest", "nomic-embed-text:latest"],
    "has_llm": true,
    "has_embedding": true
  },
  "fallback_active": true
}
```

### POST /api/ai/test
Test le provider actif :
```json
{
  "success": true,
  "provider": "ollama",
  "model": "llama3.2",
  "response": "OK",
  "duration_ms": 450
}
```

## Service AIProviderService

```php
use KDocs\Services\AIProviderService;

$ai = new AIProviderService();

// V√©rifier la disponibilit√©
if ($ai->isAIAvailable()) {
    $provider = $ai->getBestProvider(); // 'claude', 'ollama', ou 'none'
}

// Classification automatique (utilise le meilleur provider)
$result = $ai->classifyDocument($content, $filename);

// Extraction de donn√©es
$data = $ai->extractData($content, ['date', 'amount', 'reference']);

// R√©sum√©
$summary = $ai->summarize($content, 200);

// Compl√©tion libre
$response = $ai->complete("Traduis en anglais: Bonjour");
```

## Comparaison des providers

| Fonctionnalit√© | Claude | Ollama | Rules |
|----------------|--------|--------|-------|
| Classification | ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê | ‚≠ê‚≠ê‚≠ê‚≠ê | ‚≠ê‚≠ê |
| Extraction | ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê | ‚≠ê‚≠ê‚≠ê | ‚≠ê |
| R√©sum√© | ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê | ‚≠ê‚≠ê‚≠ê‚≠ê | ‚ùå |
| Chat | ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê | ‚≠ê‚≠ê‚≠ê‚≠ê | ‚ùå |
| Embeddings | ‚ùå | ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê | ‚ùå |
| Co√ªt | üí∞üí∞ | ‚úÖ Gratuit | ‚úÖ Gratuit |
| Confidentialit√© | ‚òÅÔ∏è Cloud | ‚úÖ 100% local | ‚úÖ Local |
| Vitesse | ‚ö° Rapide | üê¢ Variable | ‚ö° Instant |

## Cas d'usage

### Sc√©nario 1 : Production avec budget
- Configurer Claude API
- Ollama en backup si quota d√©pass√©
- Meilleure qualit√© garantie

### Sc√©nario 2 : Installation locale sans budget
- Ollama uniquement
- Qualit√© acceptable pour la plupart des usages
- 100% gratuit et priv√©

### Sc√©nario 3 : Environnement air-gapped
- Ollama obligatoire (pas d'internet)
- Mod√®les pr√©-t√©l√©charg√©s

## Smoke Test

```bash
php tests/smoke_test.php
```

Output attendu :
```
--- INTELLIGENCE ARTIFICIELLE ---
[OK] 29. Claude API - disponible
[OK] 30. Ollama (fallback) - disponible  
[OK] 31. Provider IA actif - Claude (qualit√© max)
```

ou si Claude non configur√© :
```
[!!] 29. Claude API - non configur√© (warning)
[OK] 30. Ollama (fallback) - disponible
[OK] 31. Provider IA actif - Ollama (fallback)
```

---

*Architecture document√©e le 30/01/2026*
